{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM data prep\n",
    "\n",
    "Create a table of data for each recording.\n",
    "Each row is a millisecond (data only from bouts).\n",
    "Variables include speeds + shifted speeds, curvature of the track, firing rate...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "from fcutils.progress import track\n",
    "from fcutils.maths import derivative\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(r\"C:\\Users\\Federico\\Documents\\GitHub\\pysical_locomotion\")\n",
    "\n",
    "\n",
    "from analysis.ephys.utils import get_recording_names, get_data, get_session_bouts\n",
    "\n",
    "save_folder = Path(r\"D:\\Dropbox (UCL)\\Rotation_vte\\Locomotion\\analysis\\ephys\")\n",
    "\n",
    "cache = Path(r\"D:\\Dropbox (UCL)\\Rotation_vte\\Locomotion\\analysis\\ephys\\GLM\\data\")\n",
    "recordings = get_recording_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curvature_horizon = 30\n",
    "curvature_sampling_spacing = 5\n",
    "curv_sample_points = np.arange(0, curvature_horizon+curvature_sampling_spacing, curvature_sampling_spacing)\n",
    "\n",
    "track_downsample_factor = 25\n",
    "\n",
    "firing_rate_gaussian = 250 # width in ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track curvature\n",
    "Sample the track curvature for N future positions given each track position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data = pd.read_json(r\"C:\\Users\\Federico\\Documents\\GitHub\\pysical_locomotion\\analysis\\ephys\\track.json\").iloc[::track_downsample_factor]\n",
    "track_data = track_data.reset_index(drop=True)\n",
    "S_f = track_data.S.values[-1]\n",
    "track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load track from json\n",
    "k_shifts = np.arange(curvature_horizon+1)\n",
    "curv_shifted = {\n",
    "    **{f\"k_{k}\":[] for k in k_shifts},\n",
    "    **{f\"idx_{k}\":[] for k in k_shifts},\n",
    "}\n",
    "for i, s in enumerate(track_data.S):\n",
    "    for k in k_shifts:\n",
    "        if s + k < S_f:\n",
    "            select = track_data.loc[track_data.S >= s + k]\n",
    "            curv_shifted[f\"idx_{k}\"].append(select.index[0])\n",
    "            curv_shifted[f\"k_{k}\"].append(select[\"curvature\"].iloc[0])\n",
    "        else:\n",
    "            curv_shifted[f\"k_{k}\"].append(np.nan)\n",
    "            curv_shifted[f\"idx_{k}\"].append(np.nan)\n",
    "\n",
    "    # break\n",
    "\n",
    "for k,v in curv_shifted.items():\n",
    "    track_data.insert(2, k, v)\n",
    "track_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsample_frames_to_ms(var):\n",
    "    \"\"\"\n",
    "        Interpolates the values of a variable expressed in frams (60 fps)\n",
    "        to values expressed in milliseconds.\n",
    "    \"\"\"\n",
    "    t_60fps = np.arange(len(var)) / 60\n",
    "    f = interpolate.interp1d(t_60fps, var)\n",
    "\n",
    "    # t_1000fps = np.arange(0, t_60fps[-1], step=1/1000)\n",
    "    t_200fps = np.arange(0, t_60fps[-1], step=1/200)\n",
    "    interpolated_variable_values = f(t_200fps)\n",
    "    return interpolated_variable_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian(x, s):\n",
    "    return (\n",
    "        1.0\n",
    "        / np.sqrt(2.0 * np.pi * s ** 2)\n",
    "        * np.exp(-(x ** 2) / (2.0 * s ** 2))\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_firing_rate(spikes_train: np.ndarray, dt: int = 10):\n",
    "    \"\"\"\n",
    "        Computes the firing rate given a spikes train (wether there is a spike or not at each ms).\n",
    "        Using a gaussian kernel with standard deviation = dt/2 [dt is in ms]\n",
    "    \"\"\"\n",
    "    # create kernel & get area under the curve\n",
    "    k = np.array([gaussian(x, dt / 2) for x in np.linspace(-2*dt, 2*dt, dt)])\n",
    "    auc = np.trapz(k)\n",
    "\n",
    "    # get firing rate\n",
    "    frate = (\n",
    "        np.convolve(spikes_train, k, mode=\"same\") / auc * 1000\n",
    "    )  # times 1000 to go from ms to seconds\n",
    "    return frate[::5]  # sample every 5 ms -> 200 fps\n",
    "    # return frate\n",
    "\n",
    "\n",
    "def make_shuffled_units(units):\n",
    "    \"\"\"\n",
    "        For each unit make shuffled copies in which \n",
    "        the firing rate is offset by some ammount looping\n",
    "        around the start/end of the session\n",
    "    \"\"\"\n",
    "    N = 100\n",
    "    shuffled_units = dict(unit_id=[], firing_rate_ms=[])\n",
    "    for i, unit in units.iterrows():\n",
    "        for n in range(N):\n",
    "            shuffle = np.random.randint(10 * 200, 30 * 200)  # shuffle between 10 and 30 seconds\n",
    "            shuffled_units[\"unit_id\"].append(f\"{unit.unit_id}_shuffle_{n}\")\n",
    "\n",
    "            frate = unit.firing_rate_ms\n",
    "            frate = np.hstack([frate[shuffle:], frate[:shuffle]])\n",
    "            shuffled_units[\"firing_rate_ms\"].append(frate)\n",
    "\n",
    "    # merge units and shuffle units in a single dataframe\n",
    "    shuffled_units = pd.DataFrame(shuffled_units)\n",
    "    units = pd.concat([units, shuffled_units], ignore_index=True)\n",
    "    return units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample\n",
    "def load_get_recording_data(REC): \n",
    "    # load data\n",
    "    units, left_fl, right_fl, left_hl, right_hl, body = get_data(REC)\n",
    "\n",
    "\n",
    "    out_bouts = get_session_bouts(REC, complete=None)\n",
    "    in_bouts = get_session_bouts(REC, direction=\"inbound\", complete=None)\n",
    "\n",
    "    v = upsample_frames_to_ms(body.speed)\n",
    "    omega = upsample_frames_to_ms(body.thetadot)\n",
    "\n",
    "    v_250ms = np.hstack([v[250:], v[250] * np.ones(250)]) - v\n",
    "    v_500ms = np.hstack([v[500:], v[500] * np.ones(500)]) - v\n",
    "    v_1000ms = np.hstack([v[1000:], v[1000] * np.ones(1000)]) - v\n",
    "\n",
    "    omega_500ms = np.hstack([omega[500:], omega[500] * np.ones(500)]) - omega\n",
    "    omega_250ms = np.hstack([omega[250:], omega[250] * np.ones(250)]) - omega\n",
    "    omega_1000ms = np.hstack([omega[1000:], omega[1000] * np.ones(1000)]) - omega\n",
    "\n",
    "\n",
    "    # get unit firing rate in milliseconds\n",
    "    units = units.loc[units.brain_region.isin([\"MOs\", \"MOs1\", \"MOs2/3\", \"MOs5\", \"MOs6a\", \"MOs6b\"])]\n",
    "    frates = []\n",
    "    for i, unit in units.iterrows():\n",
    "        time = np.zeros(len(v) * 5)  # time in milliseconds\n",
    "        spikes_times = np.int64(np.round(unit.spikes_ms))\n",
    "        spikes_times = spikes_times[spikes_times < len(time)]\n",
    "        time[spikes_times] = 1\n",
    "        frates.append(calc_firing_rate(time, dt=firing_rate_gaussian))  # firing rate at 200fps\n",
    "    units[\"firing_rate_ms\"] = frates\n",
    "    units = units[[\"unit_id\", \"firing_rate_ms\"]]  # discard unnecessary columns\n",
    "\n",
    "    # add shuffled units\n",
    "    units = make_shuffled_units(units)\n",
    "\n",
    "    return units, body, pd.concat([out_bouts, in_bouts]).reset_index(), v, omega, v_250ms, omega_250ms, v_500ms, omega_500ms, v_1000ms, omega_1000ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data for all bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for REC in recordings:\n",
    "    savepath = cache / f\"{REC}_bouts.h5\"\n",
    "    if savepath.exists():\n",
    "        print(f\"{REC}_bouts.h5 already exists\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Doing   {REC}\")\n",
    "    out_bouts = get_session_bouts(REC, complete=None)\n",
    "    in_bouts = get_session_bouts(REC, direction=\"inbound\", complete=None)\n",
    "    allbouts = pd.concat([out_bouts, in_bouts]).reset_index()\n",
    "\n",
    "    bouts_files = list(cache.glob(f\"{REC}_bout_*.h5\"))\n",
    "    if len(bouts_files) < len(allbouts):\n",
    "        print(f\"    Not all bouts were saved for {REC}\")\n",
    "        continue\n",
    "\n",
    "    bouts_data = []\n",
    "    for i, bout in allbouts.iterrows():\n",
    "        f = cache / f\"{REC}_bout_{bout.start_frame}.h5\"\n",
    "        if not f.exists():\n",
    "            break\n",
    "        bouts_data.append(pd.read_hdf(f, key=\"data\"))\n",
    "    bouts_data = pd.concat(bouts_data)\n",
    "\n",
    "    print(f\" Got all data ({bouts_data.shape}), removing outliers\")\n",
    "    bouts_data[(np.abs(stats.zscore(bouts_data)) < 3).all(axis=1)]\n",
    "\n",
    "    print(\" Saving data\")\n",
    "    bouts_data.to_hdf(savepath, key=\"data\")\n",
    "    print(\" Saved all data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, for each recording and each bout save a .h5 file. \n",
    "# Then load it back and merge them toghether. This is to avoid memory issues.\n",
    "\n",
    "for REC in recordings:\n",
    "    if (cache / f\"{REC}_bouts.h5\").exists():\n",
    "        print(f\"{REC}_bouts.h5 already exists\")\n",
    "        continue\n",
    "    print(f\"Processing {REC}\")\n",
    "\n",
    "    units, body, bouts, v, omega, v_250ms, omega_250ms, v_500ms, omega_500ms, v_1000ms, omega_1000ms = load_get_recording_data(REC)\n",
    "    print(\"     got all data\")\n",
    "\n",
    "    for i, bout in track(bouts.iterrows(), total=len(bouts), description=REC):\n",
    "        bout_savepath = cache / f\"{REC}_bout_{bout.start_frame}.h5\"\n",
    "        if bout_savepath.exists():\n",
    "            print(f\"{REC}_bout_{bout.start_frame}.h5 already exists\")\n",
    "            continue\n",
    "\n",
    "        data = {\n",
    "            **dict(\n",
    "                s=[],\n",
    "                sdot=[],\n",
    "                v=[],\n",
    "                dv_250ms=[],\n",
    "                dv_500ms=[],\n",
    "                dv_1000ms=[],\n",
    "                omega=[],\n",
    "                domega_250ms=[],\n",
    "                domega_500ms=[],\n",
    "                domega_1000ms=[],\n",
    "            ),\n",
    "            **{f\"curv_{k}cm\":[] for k in curv_sample_points},\n",
    "            **{unit:[] for unit in units.unit_id.values},\n",
    "        }\n",
    "\n",
    "\n",
    "        S = upsample_frames_to_ms(bout.s)\n",
    "        data['s'].extend(S)\n",
    "        data['sdot'].extend(derivative(S) * 60)\n",
    "\n",
    "        start_ms = int(bout.start_frame / 60 * 200)\n",
    "        end_ms = start_ms + len(S)\n",
    "        data['v'].extend(v[start_ms : end_ms])\n",
    "        data['dv_250ms'].extend(v_250ms[start_ms : end_ms])\n",
    "        data['dv_500ms'].extend(v_500ms[start_ms : end_ms])\n",
    "        data['dv_1000ms'].extend(v_1000ms[start_ms : end_ms])\n",
    "        data['omega'].extend(omega[start_ms : end_ms])\n",
    "        data['domega_250ms'].extend(omega_250ms[start_ms : end_ms])\n",
    "        data['domega_500ms'].extend(omega_500ms[start_ms : end_ms])\n",
    "        data['domega_1000ms'].extend(omega_1000ms[start_ms : end_ms])\n",
    "\n",
    "        # get firing rate\n",
    "        for i, unit in units.iterrows():\n",
    "            data[unit.unit_id].extend(unit.firing_rate_ms[start_ms : end_ms])\n",
    "\n",
    "        # get curvature\n",
    "        for k_cm in curv_sample_points:\n",
    "            for s in S:\n",
    "                idx = np.argmin((track_data.S - s)**2)\n",
    "                data[f\"curv_{k_cm}cm\"].append(track_data[f\"k_{k_cm}\"][idx])\n",
    "\n",
    "\n",
    "        # ensure all entries have the same number of samples\n",
    "        lengths = set([len(v) for v in data.values()])\n",
    "        if len(lengths) > 1:\n",
    "            lns = {k:len(v) for k,v in data.items()}        \n",
    "            raise ValueError(f\"Lengths of data are not the same:\\n{lns}\")\n",
    "\n",
    "    \n",
    "        pd.DataFrame(data).to_hdf(bout_savepath, key=\"data\")\n",
    "        del data\n",
    "\n",
    "    del units\n",
    "    del body\n",
    "    del bouts\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d14c38d234234b969aee73678d168700778a98933f098d78df9f79a7508c5a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
